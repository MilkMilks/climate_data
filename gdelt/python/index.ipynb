{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# remove non-UNIQUE URLS\n",
    "# for file in glob.glob('../data_sources/*/*.CSV'):\n",
    "# \t\tfile_name = file.split('/')[-1].split('.')[0]\n",
    "# \t\tdata = pd.read_csv(file, sep='\\t', low_memory=False, names=[\"GlobalEventID\", \"Day\", \"MonthYear\", \"Year\", \"FractionDate\", \"Actor1Code\", \"Actor1Name\", \"Actor1CountryCode\", \"Actor1KnownGroupCode\", \"Actor1EthnicCode\", \"Actor1Religion1Code\", \"Actor1Religion2Code\", \"Actor1Type1Code\", \"Actor1Type2Code\", \"Actor1Type3Code\", \"IsRootEvent\", \"EventCode\", \"EventBaseCode\", \"EventRootCode\", \"QuadClass\", \"GoldsteinScale\", \"NumMentions\", \"NumSources\", \"NumArticles\", \"AvgTone\", \"Actor1Geo_Type\", \"Actor1Geo_Fullname\", \"Actor1Geo_CountryCode\", \"Actor1Geo_ADM1Code\", \"Actor1Geo_ADM2Code\", \"Actor1Geo_Lat\", \"Actor1Geo_Long\", \"Actor1Geo_FeatureID\", \"DATEADDED\", \"SOURCEURL\"])\n",
    "# \t\tdf = data.drop_duplicates('SOURCEURL')\n",
    "# \t\tdf.to_csv(\"../fetch/parsed_data/tsv/%s.tsv\" % (file_name), index=False, header=True, sep=\"\\t\")\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#count the length of every file and update f_length\n",
    " # 27,760,521 rows\n",
    "# l = 0\n",
    "# for file in glob.glob('../fetch/parsed_data/tsv/*.tsv'):\t\n",
    "# \t\tdata = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "# \t\tl += len(data)\n",
    "\n",
    "# print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# protests = []\n",
    "# for file in glob.glob('../fetch/parsed_data/tsv/*.tsv'):\n",
    "# \tdata = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "# \tdata = data[data['SOURCEURL'].str.contains('climate') & data['SOURCEURL'].str.contains('protest')]\n",
    "# \tfor index, row in data.iterrows():\n",
    "# \t\tif pd.isna(row['Actor1Geo_Lat']) == False:\n",
    "# \t\t\tprotests.append([row['DATEADDED'], row['Actor1Geo_Lat'], row['Actor1Geo_Long'], row['SOURCEURL']])\n",
    "# \ti += 1\n",
    "# \tif i % 5000 == 0:\n",
    "# \t\tprint(i)\n",
    "\t\t\n",
    "# # print(len(protests)) # 5,244\n",
    "\n",
    "# df = pd.DataFrame(protests, columns=['DATEADDED', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'SOURCEURL'])\n",
    "# df.to_csv(\"protests.tsv\", index=False, header=True, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milk/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/home/milk/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/home/milk/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname MDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/home/milk/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname CDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/home/milk/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from newspaper import Article\n",
    "id = 0\n",
    "csv = pd.read_csv('../fetch/parsed_data/protests.tsv', sep='\\t', index_col=False, header=None, names=['date', 'lat', 'long', 'url'])\n",
    "for row in csv.iterrows():\n",
    "    try:\n",
    "        row = row[1]\n",
    "        date = row['date']\n",
    "        lat = row['lat']\n",
    "        long = row['long']\n",
    "        url = row['url']\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        df = {\"id\": id, \"date\": date, \"lat\": lat,\"long\": long,\"URL\": url, \"TEXT\": article.text}\n",
    "        location = \"../fetch/parsed_data/articles_json/%s.json\" % (id)\n",
    "        df = [json.dump(df, open(location, 'w'))]\n",
    "    except Exception as e: \n",
    "        #DO NOTHING\n",
    "        pass\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eaed1786ec1d7a4a2f9c9e62e67b4aaf30f188089613b3f2b988b705566edad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
